{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "b7443374",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# from utils.torch_utils import prune\n",
    "from utils.torch_utils import select_device\n",
    "from models.experimental import attempt_load\n",
    "\n",
    "from utils.plots import plot_one_box\n",
    "from utils.general import check_img_size, check_requirements, check_imshow, non_max_suppression\n",
    "from utils.general import apply_classifier, scale_coords, xyxy2xywh, strip_optimizer, set_logging, increment_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "623cd240",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune(model, amount=0.3):\n",
    "    # Prune model to requested global sparsity\n",
    "    import torch.nn.utils.prune as prune\n",
    "    print('Pruning model... ', end='')\n",
    "    for name, m in model.named_modules():\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            prune.l1_unstructured(m, name='weight', amount=amount)  # prune\n",
    "            prune.remove(m, 'weight')  # make permanent\n",
    "            \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "0aa2c628",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fusing layers... \n",
      "RepConv.fuse_repvgg_block\n",
      "RepConv.fuse_repvgg_block\n",
      "RepConv.fuse_repvgg_block\n",
      "IDetect.fuse\n"
     ]
    }
   ],
   "source": [
    "device = '0'\n",
    "weights = './runs/train/yolov7-custom/weights/best.pt'\n",
    "\n",
    "device = select_device(device)\n",
    "model = attempt_load(weights, map_location=device)  # load FP32 model\n",
    "# stride = int(model.stride.max())  # model stride\n",
    "\n",
    "if device.type != 'cpu':\n",
    "    model(torch.zeros(1, 3, 640, 640).to(device).type_as(next(model.parameters())))  # run once\n",
    "\n",
    "# model = prune(model)\n",
    "\n",
    "old_img_b = 4\n",
    "old_img_h = 640\n",
    "old_img_w = 640\n",
    "\n",
    "img = cv2.imread('test.png')\n",
    "img = img[...,::-1]\n",
    "img = cv2.resize(img, (640,640))\n",
    "\n",
    "img = torch.from_numpy(img).to(device)\n",
    "img = img.unsqueeze(0)\n",
    "img = torch.moveaxis(img, -1, 1)\n",
    "img = img.float()\n",
    "img /= 255.0\n",
    "\n",
    "for i in range(3):\n",
    "    model(img, augment=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "517de33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def centeroid(a):\n",
    "    length = a.shape[0]\n",
    "    sum_x = np.sum(a[:, 0])\n",
    "    sum_y = np.sum(a[:, 1])\n",
    "    return int(sum_x/length), int(sum_y/length)\n",
    "\n",
    "def cluster(centers):\n",
    "    dist = np.array([np.linalg.norm(centers[0]-center) for center in centers])\n",
    "    clusters = [0] * len(centers)\n",
    "    if len(centers) > 1:\n",
    "        if np.max(dist) > 250:\n",
    "            kmeans = KMeans(n_clusters=2, n_init=5).fit(centers)\n",
    "            clusters = list(kmeans.labels_)\n",
    "\n",
    "        if 1 in clusters:\n",
    "            ind0 = clusters.index(0)\n",
    "            ind1 = clusters.index(1)\n",
    "            clusters = np.array(clusters)\n",
    "            if centers[ind0, 0] > centers[ind1, 0]:\n",
    "                clusters = 1 - clusters\n",
    "\n",
    "    return np.array(clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "69735c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [[255,0,0], [0,0,255]]\n",
    "\n",
    "cap = cv2.VideoCapture(0, cv2.CAP_V4L2)\n",
    "cap.set(cv2.CAP_PROP_FOURCC, cv2.VideoWriter_fourcc('M', 'J', 'P', 'G'))\n",
    "width = 1920\n",
    "height = 1080\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, width)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, height)    \n",
    "while(True):\n",
    "      \n",
    "    ret, img = cap.read()\n",
    "    \n",
    "    img = cv2.flip(img, 1)\n",
    "    img0 = img.copy()\n",
    "    img = img[...,::-1]\n",
    "    img = cv2.resize(img, (640,640))\n",
    "\n",
    "    img = torch.from_numpy(img).to(device)\n",
    "    img = img.unsqueeze(0)\n",
    "    img = torch.moveaxis(img, -1, 1)\n",
    "    img = img.float()\n",
    "    img /= 255.0\n",
    "    \n",
    "    with torch.no_grad():   # Calculating gradients would cause a GPU memory leak\n",
    "        pred = model(img, False)[0]\n",
    "\n",
    "    conf_thres = 0.1\n",
    "    iou_thres = 0.01\n",
    "    pred = non_max_suppression(pred, conf_thres, iou_thres, classes=0, agnostic=True)\n",
    "    \n",
    "    for i, det in enumerate(pred):\n",
    "        if len(det):\n",
    "            det[:, :4] = scale_coords(img.shape[2:], det[:, :4], img0.shape).round()\n",
    "            \n",
    "#             det_np = det.clone().cpu().numpy()\n",
    "#             centers = np.array([det_np[:,0] + (det_np[:,2]-det_np[:,0]) // 2, \n",
    "#                                 det_np[:,1] + (det_np[:,3]-det_np[:,1]) // 2])\n",
    "#             centers = np.transpose(centers)\n",
    "#             clusters = cluster(centers.astype(np.float32))\n",
    "            \n",
    "#             if len(clusters) > 0:\n",
    "#                 centeroidA = centeroid(centers[clusters==0])\n",
    "#                 if 1 in clusters:\n",
    "#                     centeroidB = centeroid(centers[clusters==1])\n",
    "                \n",
    "#                 cv2.circle(img0, centeroidA, 5, (0, 0, 255), 5)\n",
    "#                 # cv2.circle(frame, centeroidA, 5, (0, 0, 255), 5)\n",
    "#                     # if clusters[i] == 1:\n",
    "#                 if 1 in clusters:\n",
    "#                     cv2.circle(img0, centeroidB, 5, (255, 0, 0), 5)\n",
    "\n",
    "            \n",
    "#             ct = 0\n",
    "            for *xyxy, conf, cls in reversed(det):\n",
    "#                 color = colors[clusters[ct]]\n",
    "                plot_one_box(xyxy, img0, label=None, color=color, line_thickness=1)\n",
    "#                 ct += 1\n",
    "    \n",
    "    cv2.imshow('frame', img0)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "297fe14a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[       1737,        1080],\n",
       "       [       1740,        1080],\n",
       "       [       1738,        1080],\n",
       "       [       1737,        1080],\n",
       "       [       1738,        1080],\n",
       "       [       1737,        1080],\n",
       "       [       1738,        1080],\n",
       "       [       1737,        1080],\n",
       "       [       1737,        1080]], dtype=float32)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f1667dd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(183, 1080)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "centeroidA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6a4e2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
